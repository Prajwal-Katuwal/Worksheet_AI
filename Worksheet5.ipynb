{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cxBXYY0qUglEajN5dPilP-TcOhwqLazC",
      "authorship_tag": "ABX9TyPDlaFLcJKgSkGyL5X+IBru"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Datasets/student.csv\")\n",
        "\n",
        "print(data.head())\n",
        "print(data.tail())\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "\n",
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPoN8V6Wd0gZ",
        "outputId": "00254b03-18a3-4a7d-c640-dab29d4cab5d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = X.T          # shape: (d, n)\n",
        "Y = Y            # shape: (n,)\n",
        "W = np.zeros(X.shape[0])  # shape: (d,)\n"
      ],
      "metadata": {
        "id": "r7ruS8E5d90M"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X.T, Y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "SMRiCq1jd_w3"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_vGF5RbX4-7j"
      },
      "outputs": [],
      "source": [
        "#Define the cost function\n",
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "  \"\"\"Parameters:\n",
        "This function finds the Mean Square Error.\n",
        "Input Parameters:\n",
        "X: Feature Matrix\n",
        "Y: Target Matrix\n",
        "W: Weight Matrix\n",
        "OUtpit Parameters:\n",
        "cost: accumulated mean square error.\n",
        "\"\"\"\n",
        "  m = len(Y)\n",
        "  y_pred = X.dot(W)\n",
        "  cost = (1/m) * np.sum((y_pred - Y)**2)\n",
        "  return cost\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test case\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "if cost == 0:\n",
        "    print(\"Proceed further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement cost function\")\n",
        "\n",
        "print(\"Cost function output:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chlTwrNe-lfg",
        "outputId": "c2c5d813-af34-46dc-a210-8849c029494e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "    W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "    cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W - alpha * dw\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "        # IMPORTANT: update W for next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n"
      ],
      "metadata": {
        "id": "DxP6xBu9EHX0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kk_2JJzmfO24"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Squared Error.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (Target) dependent variables.\n",
        "    Y_pred: Array of predicted dependent variables.\n",
        "\n",
        "    Output:\n",
        "    rmse: Root Mean Squared Error\n",
        "    \"\"\"\n",
        "    rmse = np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "    return rmse\n"
      ],
      "metadata": {
        "id": "3OlM-vD6hF6l"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the R Squared Error.\n",
        "\n",
        "    Input:\n",
        "    Y: Array of actual (Target) dependent variables.\n",
        "    Y_pred: Array of predicted dependent variables.\n",
        "\n",
        "    Output:\n",
        "    r2: R Squared Error\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "2hv0ZskphQkj"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Datasets/student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values            # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.00001      # Learning rate\n",
        "    iterations = 1000    # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwNPrPVvhYMV",
        "outputId": "0bedeb67-71b6-489f-d740-ca69d45baec7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [np.float64(4026.33114156751), np.float64(3280.573665199384), np.float64(2674.1239989803175), np.float64(2180.9589785701155), np.float64(1779.9166540166468), np.float64(1453.788198601909), np.float64(1188.5794521617188), np.float64(972.910410590327), np.float64(797.5268927198968), np.float64(654.9034294649376)]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    }
  ]
}